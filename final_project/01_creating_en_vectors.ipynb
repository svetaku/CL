{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "stemmer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D:\\\\python\\\\CL\\\\gob\\\\pi2.csv', sep=';', low_memory=False)\n",
    "data = data[['Russian', 'English']]\n",
    "index_names = data[data['Russian'].str.contains(\"тест\" or \"test\")==True].index\n",
    "data.drop(index_names, inplace = True)\n",
    "data = data.dropna()\n",
    "data = data.drop_duplicates(subset=['English', 'Russian'])\n",
    "data = data[data.Russian != data.English]\n",
    "data_en = data['English'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(item):\n",
    "    item = re.sub(r\"\\d+%\", \" \", item)\n",
    "    item = re.sub(r\"x\\d+\", \" \", item)\n",
    "    item = re.sub(r\"\\d+\", \" \", item)\n",
    "    item = re.sub(r\"\\n\", \" \", item)\n",
    "    item = re.sub(r\"\\[.+\\]\", \" \", item)\n",
    "    item = re.sub(r\"\\\\+.+;\", \" \", item)\n",
    "    item = re.sub(r\"http.+\", \" \", item)\n",
    "    item = re.sub(r\"\\{.*\\}\", \" \", item)\n",
    "    item = re.sub(r\" [xX] \", \" \", item)\n",
    "    item = re.sub(r\"%[sd]\", \" \", item)\n",
    "    item = re.sub(r\"<.+>\", \" \", item)\n",
    "    item = re.sub(r\"[\\U00010000-\\U0010ffff]\", \" \", item)\n",
    "    item = re.sub(r\"[!@#$%\\^\\&\\*()_=+\\?\\!:;\\\",\\.\\\\»«—-]\", \" \", item)\n",
    "    item = re.sub(r\"\\s+\", \" \", item)\n",
    "    item = item.strip(' ')\n",
    "    item = item.lower()\n",
    "    \n",
    "    tokens = item.split()\n",
    "    tokens = [nlp(word)[0].lemma_ if word != \"flowerbed\" else \"flowerbed\" for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus = [preprocess_text(sentence) for sentence in data_en if sentence.strip() !='']\n",
    "\n",
    "word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n",
    "word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 15\n",
    "window_size = 60\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n",
    "\n",
    "ft_model = FastText(word_tokenized_corpus,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      sg=1,\n",
    "                      iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3109057   0.10911747 -0.17765515 -1.2655255  -0.30690217  0.73108137\n",
      " -0.9664295   0.19486967 -0.81640035  1.0871066   0.5006442  -0.6488135\n",
      "  1.1173453   0.5123931  -0.86224246  0.3509511   1.8848457  -0.42018625\n",
      "  0.59783083 -0.625262    0.62150514  1.252173   -0.1979469   0.493237\n",
      " -0.03667224 -0.9095915  -0.39532563 -1.1574185   0.03999989  1.137751\n",
      "  0.05025969 -1.2376442  -0.4071073   0.28774914 -0.77259207 -1.1513922\n",
      " -1.3808985  -0.01335795 -0.6939756   1.2324158   0.14021489 -0.01564856\n",
      " -0.39438525 -0.4661787   0.32009938  0.0781133  -0.2658846   0.5526048\n",
      " -0.6981289  -0.89133275  0.25068483  0.25183654 -0.08277569 -0.3388166\n",
      " -0.40359524 -1.5942771   0.5266754  -0.6431995   0.29430228  1.3802017 ]\n"
     ]
    }
   ],
   "source": [
    "print(ft_model.wv['piñata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crystal:['buy', 'wondershop', 'purchase', 'currency', 'bank']\n",
      "naomi:['come', 'say', 'cause', 'suppose', 'since']\n",
      "key:['lock', 'purr', 'mrrr', 'vial', 'odd']\n",
      "island:['set', 'paradise', 'micro', 'ultrabonus', 'middle']\n",
      "build:['upgrade', 'building', 'eco', 'entertainment', 'instal']\n",
      "reward:['prize', 'awesome', 'earn', 'willy', 'get']\n"
     ]
    }
   ],
   "source": [
    "semantically_similar_words = {words: [item[0] for item in ft_model.wv.most_similar([words], topn=5)]\n",
    "                  for words in ['crystal', 'naomi', 'key', 'island', 'build', 'reward']}\n",
    "\n",
    "for k,v in semantically_similar_words.items():\n",
    "    print(k+\":\"+str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8932379\n",
      "0.87251574\n",
      "0.60690033\n"
     ]
    }
   ],
   "source": [
    "print(ft_model.wv.similarity(w1='dorin', w2='mano'))\n",
    "print(ft_model.wv.similarity(w1='event', w2='hotel'))\n",
    "print(ft_model.wv.similarity(w1='gold', w2='coin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import save_facebook_model\n",
    "save_facebook_model(ft_model, \"en_model_fb.bin\", encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
