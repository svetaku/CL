{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D:\\\\python\\\\CL\\\\gob\\\\pi222.csv', sep=';', low_memory=False)\n",
    "data = data[['Russian', 'English']]\n",
    "index_names = data[data['Russian'].str.contains(\"тест\" or \"test\")==True].index\n",
    "data.drop(index_names, inplace = True)\n",
    "data = data.dropna()\n",
    "data = data[data.Russian != data.English]\n",
    "data = data.drop_duplicates(subset=['English', 'Russian'])\n",
    "data_ru = data['Russian'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(item):\n",
    "    item = re.sub(r\"\\d+%\", \" \", item)\n",
    "    item = re.sub(r\"x\\d+\", \" \", item)\n",
    "    item = re.sub(r\"\\d+\", \" \", item)\n",
    "    item = re.sub(r\"\\n\", \" \", item)\n",
    "    item = re.sub(r\"\\[.+\\]\", \" \", item)\n",
    "    item = re.sub(r\"\\\\.+\\\\;\", \" \", item)\n",
    "    item = re.sub(r\"http.+\", \" \", item)\n",
    "    item = re.sub(r\"\\{.*\\}\", \" \", item)\n",
    "    item = re.sub(r\" [xX] \", \" \", item)\n",
    "    item = re.sub(r\"%[sd]\", \" \", item)\n",
    "    item = re.sub(r\"<.+>\", \" \", item)\n",
    "    item = re.sub(r\"[\\U00010000-\\U0010ffff]\", \" \", item)\n",
    "    item = re.sub(r\"[!@#$%\\^\\&\\*()_=+\\?\\!:;\\\",\\.\\\\»«—№]\", \" \", item)\n",
    "    item = re.sub(r\"\\s+\", \" \", item)\n",
    "    item = item.strip(' ')\n",
    "    item = item.lower()\n",
    "    \n",
    "    tokens = item.split()\n",
    "    tokens = [morph.parse(word)[0].normal_form for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('russian')]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus = [preprocess_text(sentence) for sentence in data_ru if sentence.strip() !='']\n",
    "\n",
    "word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n",
    "word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 15\n",
    "window_size = 60\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n",
    "\n",
    "ft_model = FastText(word_tokenized_corpus,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      sg=1,\n",
    "                      iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8197248  -0.2606538   0.09210553  0.4402155  -0.90772134  0.3352683\n",
      " -1.8322273   0.1421681  -1.0818759   0.64429766  0.84142375 -0.5680857\n",
      " -0.01411205 -0.5607831   0.41943598 -0.7892568  -0.34489664  1.1795076\n",
      " -0.5877148   0.31188834 -0.46960866  0.53191334  0.39933002  0.08426717\n",
      "  0.9956709   0.5206764  -0.5992941  -0.2144051  -0.96196294 -0.6895037\n",
      "  0.5823305   0.25126904  0.07979515 -0.27547386  0.30598214  0.7957636\n",
      " -0.21333589  0.3610347  -0.7073324  -0.02736245  0.87782854 -0.38474396\n",
      " -0.42535922  0.33477142  0.30496755 -0.7367027   0.54787797  0.5072618\n",
      " -1.1048433  -0.88642305 -0.877676   -0.5686525   0.03365183 -0.33665943\n",
      " -0.08682787 -1.495623   -1.1425389  -1.8019029   0.4931613   0.25050402]\n"
     ]
    }
   ],
   "source": [
    "print(ft_model.wv['ключ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кристалл:['купить', 'ресурс', 'лавка', 'полно', 'докупить']\n",
      "наоми:['казаться', 'решить', 'мано', 'забирать', 'рука']\n",
      "ключ:['старинный', 'представитель', 'лежать', 'цивилизация', 'наслать']\n",
      "остров:['решить', 'отметить', 'всё', 'начать', 'должный']\n",
      "строить:['условие', 'поднебесный', 'аксессуар', 'пятизвёздочный', 'обладатель']\n",
      "награда:['приз', 'отличный', 'получать', 'аплодисменты', 'стать']\n"
     ]
    }
   ],
   "source": [
    "semantically_similar_words = {words: [item[0] for item in ft_model.wv.most_similar([words], topn=5)]\n",
    "                  for words in ['кристалл', 'наоми', 'ключ', 'остров', 'строить', 'награда']}\n",
    "\n",
    "for k,v in semantically_similar_words.items():\n",
    "    print(k+\":\"+str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9062874\n",
      "0.78880155\n",
      "0.60436195\n"
     ]
    }
   ],
   "source": [
    "print(ft_model.wv.similarity(w1='мано', w2='дорин'))\n",
    "print(ft_model.wv.similarity(w1='отель', w2='событие'))\n",
    "print(ft_model.wv.similarity(w1='ключ', w2='сундук'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import save_facebook_model\n",
    "save_facebook_model(ft_model, \"ru_model_fb.bin\", encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
